import os
import logging
import json
from datetime import datetime
from queue import Queue
from typing import List, Tuple, Dict, Any, Callable, Generator
from flask import Response
from modules.prcs_flow import create_nmap_output_template, merge_nmap_output_template, ProcessingError
from modules.prcs_shp import process_zip
from modules.prcs_geojson import process_geojson
from modules.prcs_gpx import process_gpx
from modules.prcs_kml import process_kml
from modules.prcs_topojson import process_topojson
from modules.prcs_wkt import process_wkt
from modules.prcs_upload import (
    download_index_json,
    upload_index_json,
    ensure_folder,
    get_current_day_folder_path,
    BASE_FOLDER_PATH
)

logger = logging.getLogger(__name__)

ALLOWED_EXTENSIONS = {'zip', 'geojson', 'gpx', 'kml', 'kmz', 'topojson', 'wkt'}

FILE_PROCESSORS: Dict[str, Tuple[Callable[[str], Dict[str, Any]], str]] = {
    '.zip': (process_zip, 'Shapefile'),
    '.geojson': (process_geojson, 'GeoJSON'),
    '.gpx': (process_gpx, 'GPX'),
    '.kml': (process_kml, 'KML/KMZ'),
    '.kmz': (process_kml, 'KML/KMZ'),
    '.topojson': (process_topojson, 'TopoJSON'),
    '.wkt': (process_wkt, 'WKT'),
}


class QueueHandler(logging.Handler):

    def __init__(self, log_queue: Queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record: logging.LogRecord) -> None:
        log_entry = {
            'time': datetime.now().strftime('%H:%M:%S'),
            'level': record.levelname.lower(),
            'message': self.format(record)
        }
        self.log_queue.put(log_entry)


def allowed_file(filename: str) -> bool:
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def _get_file_extension(filename: str) -> str:
    return '.' + filename.rsplit('.', 1)[1].lower() if '.' in filename else ''


def _setup_logging(log_queue: Queue) -> QueueHandler:
    queue_handler = QueueHandler(log_queue)
    queue_handler.setLevel(logging.INFO)
    queue_handler.setFormatter(logging.Formatter('%(message)s'))
    logger.addHandler(queue_handler)
    return queue_handler


def _ensure_storage_folders() -> None:
    logger.info("ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ð¿Ð°Ð¿ÐºÐ¸ Ð² Ð‘Ð»Ð¾ÐºÐ½Ð¾Ñ‚Ðµ ÐºÐ°Ñ€Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð°")
    ensure_folder(BASE_FOLDER_PATH)
    logger.info("âœ“ Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ Ð¿Ð°Ð¿ÐºÐ° ÐµÑÑ‚ÑŒ")

    logger.info("ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ Ð¿Ð°Ð¿ÐºÐ¸ Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð´Ð°Ñ‚Ñ‹")
    ensure_folder(get_current_day_folder_path())
    logger.info("âœ“ ÐŸÐ°Ð¿ÐºÐ° Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð´Ð°Ñ‚Ñ‹ ÐµÑÑ‚ÑŒ")


def _load_current_index() -> Dict[str, Any]:
    logger.info("Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ñ„Ð°Ð¹Ð»Ð° index.json")
    current_index = download_index_json()

    if current_index is None:
        current_index = create_nmap_output_template()
        logger.info("Ð¡Ð¾Ð·Ð´Ð°Ð½ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» index.json")
    else:
        logger.info("âœ“ Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½")

    return current_index


def _process_single_file(temp_path: str, filename: str) -> Dict[str, Any]:
    extension = _get_file_extension(filename)

    if extension not in FILE_PROCESSORS:
        raise ValueError(f"ÐÐµÐ¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ð¹ Ñ‚Ð¸Ð¿ Ñ„Ð°Ð¹Ð»Ð°: {extension}")

    processor, format_name = FILE_PROCESSORS[extension]
    logger.info(f"ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð¸ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ {format_name}")

    return processor(temp_path)


def process_upload_async(log_queue: Queue, session_id: str, temp_files: List[Tuple[str, str]]) -> None:
    queue_handler = _setup_logging(log_queue)

    try:
        if not temp_files:
            logger.error("ÐÐµ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ñ‹ Ñ„Ð°Ð¹Ð»Ñ‹ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸")
            return

        try:
            _ensure_storage_folders()
        except ProcessingError as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¯Ð½Ð´ÐµÐºÑ.Ð”Ð¸ÑÐºÐ°: {e.message}")
            return

        try:
            current_index = _load_current_index()
        except ProcessingError as e:
            logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ñ„Ð°Ð¹Ð» index.json: {e.message}")
            return

        new_data = create_nmap_output_template()
        logger.info(f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° {len(temp_files)} Ñ„Ð°Ð¹Ð»(Ð¾Ð²)")

        processed_count = 0
        skipped_count = 0

        for temp_path, filename in temp_files:
            logger.info(f"ðŸ“„ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°: {filename}")

            try:
                result = _process_single_file(temp_path, filename)
                new_data = merge_nmap_output_template(new_data, result)
                logger.info(f"âœ“ {filename} ÑÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð² index.json")
                processed_count += 1

            except ProcessingError as e:
                logger.error(f"âœ— {filename}: {e.message}")
                skipped_count += 1
            except ValueError as e:
                logger.error(f"âœ— {filename}: {str(e)}")
                skipped_count += 1
            except Exception as e:
                logger.error(f"âœ— {filename}: ÐÐµÐ¾Ð¶Ð¸Ð´Ð°Ð½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° - {str(e)}")
                skipped_count += 1
            finally:
                if os.path.exists(temp_path):
                    os.remove(temp_path)

        if processed_count > 0:
            logger.info("Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð² Ð‘Ð»Ð¾ÐºÐ½Ð¾Ñ‚ ÐºÐ°Ñ€Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð°")
            try:
                final_index = merge_nmap_output_template(current_index, new_data)
                upload_index_json(final_index)
                logger.info("âœ“ Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½")
            except ProcessingError as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ: {e.message}")

        logger.info(f"Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾: {processed_count} ÑƒÑÐ¿ÐµÑˆÐ½Ð¾, {skipped_count} Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾")

    finally:
        logger.removeHandler(queue_handler)
        log_queue.put(None)


def create_sse_stream(session_id: str, log_queues: Dict[str, Queue]) -> Response:
    def generate() -> Generator[str, None, None]:
        if session_id not in log_queues:
            return

        log_queue = log_queues[session_id]

        while True:
            log_entry = log_queue.get()

            if log_entry is None:
                if session_id in log_queues:
                    del log_queues[session_id]
                break

            yield f"data: {json.dumps(log_entry)}\n\n"

    return Response(generate(), mimetype='text/event-stream')
